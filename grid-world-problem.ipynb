{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Create the FrozenLake environment (as a Grid World substitute)\n",
    "env_grid = gym.make(\"FrozenLake-v1\", is_slippery=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Grid World - Value Iteration): [1. 2. 1. 0. 1. 0. 1. 0. 2. 1. 1. 0. 0. 2. 2. 0.]\n",
      "Value Table (Grid World - Value Iteration): [0.95099005 0.96059601 0.970299   0.96059601 0.96059601 0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaaddii/Documents/ai/reinforcement/.env/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(env, gamma=0.99, theta=1e-9):\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.observation_space.n):\n",
    "            Q_values = np.zeros(env.action_space.n)\n",
    "            for action in range(env.action_space.n):\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    Q_values[action] += prob * (reward + gamma * value_table[next_state])\n",
    "            max_value = np.max(Q_values)\n",
    "            delta = max(delta, np.abs(max_value - value_table[state]))\n",
    "            value_table[state] = max_value\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.zeros(env.observation_space.n)\n",
    "    for state in range(env.observation_space.n):\n",
    "        Q_values = np.zeros(env.action_space.n)\n",
    "        for action in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                Q_values[action] += prob * (reward + gamma * value_table[next_state])\n",
    "        policy[state] = np.argmax(Q_values)\n",
    "    return policy, value_table\n",
    "\n",
    "policy_grid_vi, value_table_grid = value_iteration(env_grid)\n",
    "print(\"Optimal Policy (Grid World - Value Iteration):\", policy_grid_vi)\n",
    "print(\"Value Table (Grid World - Value Iteration):\", value_table_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Grid World - Policy Iteration): [1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
      "Value Table (Grid World - Policy Iteration): [0.95099005 0.96059601 0.970299   0.96059601 0.96059601 0.\n",
      " 0.9801     0.         0.970299   0.9801     0.99       0.\n",
      " 0.         0.99       1.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(env, gamma=0.99):\n",
    "    policy = np.random.choice(env.action_space.n, env.observation_space.n)\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        # Policy Evaluation\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in range(env.observation_space.n):\n",
    "                action = policy[state]\n",
    "                value = 0\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    value += prob * (reward + gamma * value_table[next_state])\n",
    "                delta = max(delta, np.abs(value - value_table[state]))\n",
    "                value_table[state] = value\n",
    "            if delta < 1e-9:\n",
    "                break\n",
    "        # Policy Improvement\n",
    "        policy_stable = True\n",
    "        for state in range(env.observation_space.n):\n",
    "            old_action = policy[state]\n",
    "            action_values = np.zeros(env.action_space.n)\n",
    "            for action in range(env.action_space.n):\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    action_values[action] += prob * (reward + gamma * value_table[next_state])\n",
    "            new_action = np.argmax(action_values)\n",
    "            if old_action != new_action:\n",
    "                policy_stable = False\n",
    "            policy[state] = new_action\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return policy, value_table\n",
    "\n",
    "policy_grid_pi, value_table_grid = policy_iteration(env_grid)\n",
    "print(\"Optimal Policy (Grid World - Policy Iteration):\", policy_grid_pi)\n",
    "print(\"Value Table (Grid World - Policy Iteration):\", value_table_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Grid World - Q-Learning): [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Q-Table (Grid World - Q-Learning): [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def q_learning(env, episodes=1000, alpha=0.1, gamma=0.99, epsilon=0.1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])\n",
    "            state = next_state\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    return policy, Q\n",
    "\n",
    "policy_grid_ql, Q_table_grid = q_learning(env_grid)\n",
    "print(\"Optimal Policy (Grid World - Q-Learning):\", policy_grid_ql)\n",
    "print(\"Q-Table (Grid World - Q-Learning):\", Q_table_grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Action (Epsilon-Greedy): 0\n"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(len(Q[state]))\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "# Example usage\n",
    "state = env_grid.reset()[0]\n",
    "action = epsilon_greedy_policy(Q_table_grid, state, epsilon=0.1)\n",
    "print(\"Selected Action (Epsilon-Greedy):\", action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38611/2724053889.py:3: RuntimeWarning: invalid value encountered in sqrt\n",
      "  ucb_values = Q[state] + c * np.sqrt(np.log(total_counts) / (N[state] + 1e-10))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Grid World - UCB): [2 2 1 0 1 0 1 0 2 2 1 0 0 2 2 0]\n",
      "Q-Table (Grid World - UCB): [[0.00838082 0.00691285 0.95099005 0.00838082]\n",
      " [0.00717478 0.         0.96059601 0.00931946]\n",
      " [0.00701678 0.970299   0.00232261 0.01319199]\n",
      " [0.00581283 0.         0.00165103 0.00165103]\n",
      " [0.00524838 0.00884098 0.         0.00636597]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.9801     0.         0.00709953]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00533594 0.         0.01384328 0.0032767 ]\n",
      " [0.00311929 0.0176995  0.03228587 0.        ]\n",
      " [0.00801649 0.99       0.         0.970299  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.00831027 0.46414632 0.00309614]\n",
      " [0.11611211 0.99       1.         0.9801    ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "def ucb_selection(Q, N, state, c=1):\n",
    "    total_counts = np.sum(N[state]) + 1e-10\n",
    "    ucb_values = Q[state] + c * np.sqrt(np.log(total_counts) / (N[state] + 1e-10))\n",
    "    return np.argmax(ucb_values)\n",
    "\n",
    "def ucb_learning(env, episodes=1000, alpha=0.1, gamma=0.99, c=1):\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    N = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = ucb_selection(Q, N, state, c)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[next_state, best_next_action] - Q[state, action])\n",
    "            N[state, action] += 1\n",
    "            state = next_state\n",
    "    policy = np.argmax(Q, axis=1)\n",
    "    return policy, Q\n",
    "\n",
    "policy_grid_ucb, Q_table_grid = ucb_learning(env_grid)\n",
    "print(\"Optimal Policy (Grid World - UCB):\", policy_grid_ucb)\n",
    "print(\"Q-Table (Grid World - UCB):\", Q_table_grid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
